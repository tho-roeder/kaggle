{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow import keras\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"df_train=pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","metadata":{"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"independent=df_train.iloc[:,1::]\ndependent=df_train.iloc[:,:1]\ndependent_vec=keras.utils.to_categorical(dependent,10)\nindependent_test=df_test","metadata":{"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"print(dependent.value_counts())\nprint(dependent_vec[2])","metadata":{"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"label\n1        4684\n7        4401\n3        4351\n9        4188\n2        4177\n6        4137\n0        4132\n4        4072\n8        4063\n5        3795\ndtype: int64\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"}]},{"cell_type":"code","source":"def sizeImage(df):\n    import pandas as pd\n    import numpy as np\n    df=df/255.0\n    lst=[]\n    for i in range(len(df)):\n        lst.append(df.iloc[i].to_numpy().reshape(28,28))\n    #return pd.DataFrame(lst)\n    return np.array(lst)","metadata":{"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"independent_sized=sizeImage(independent)\nprint(independent_sized.shape)\nprint(independent_sized[0])\nplt.imshow(independent_sized[0])","metadata":{"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"(42000, 28, 28)\n[[0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.7372549  1.         0.36862745 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.74901961 0.98039216 0.99215686 0.36470588 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.48235294 0.97254902 0.99215686 0.65490196 0.03921569 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.31372549\n  0.96862745 0.99215686 0.81568627 0.05098039 0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.11372549 0.81176471\n  0.99215686 0.92156863 0.30196078 0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.21176471 0.81960784 0.99215686\n  0.99215686 0.34509804 0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.36470588 0.99607843 0.99215686 0.93333333\n  0.66666667 0.06666667 0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.09019608 0.82352941 0.99607843 0.99215686 0.62352941\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.0627451  0.81960784 0.99215686 0.99607843 0.94117647 0.31764706\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.10588235 0.99215686 0.99215686 0.99607843 0.05098039 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.07843137\n  0.80784314 0.99607843 0.99607843 0.77647059 0.02745098 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.65882353\n  0.99215686 0.99215686 0.76862745 0.02745098 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.07843137 0.79607843\n  0.99215686 0.97254902 0.29803922 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.08627451 0.7372549  0.99215686\n  0.96078431 0.36470588 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.40392157 0.99215686 0.99215686\n  0.74901961 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.34901961 0.94117647 0.99215686 0.76470588\n  0.09803922 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.05882353 0.8627451  0.99215686 0.99215686 0.31372549\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.36862745 0.99215686 0.99215686 0.99215686 0.36862745\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.34901961 0.98431373 0.99215686 0.98039216 0.51372549\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.83921569 0.85490196 0.37254902 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]]\n","output_type":"stream"},{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f8a76427b90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"independent_test_sized=sizeImage(independent_test)\nprint(independent_test_sized.shape)\nprint(independent_test_sized[0])\nplt.imshow(independent_test_sized[0])","metadata":{"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"(28000, 28, 28)\n[[0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.03921569 0.06666667\n  0.06666667 0.06666667 0.06666667 0.31764706 0.70588235 0.70588235\n  0.1372549  0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.54509804 0.99215686\n  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n  0.18823529 0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.23529412 0.89411765 0.99215686\n  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n  0.81176471 0.77254902 0.18039216 0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.83529412 0.99215686 0.99215686\n  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n  0.99215686 0.99215686 0.8745098  0.20392157 0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.25882353 0.90588235 0.99215686\n  0.99215686 0.99215686 0.42352941 0.15686275 0.15686275 0.45098039\n  0.95686275 0.99215686 0.99215686 0.5254902  0.01176471 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.24705882 0.44705882\n  0.44705882 0.44705882 0.14509804 0.         0.         0.\n  0.80392157 0.99215686 0.99215686 0.99215686 0.05882353 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.22352941 0.99215686 0.99215686 0.99215686 0.05882353 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.16470588 0.99215686 0.99215686 0.99215686 0.05882353 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.37254902 0.99215686 0.99215686 0.99215686 0.05882353 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.80392157 0.99215686 0.99215686 0.99215686 0.05882353 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.23921569 0.38823529 0.37647059 0.         0.         0.17647059\n  0.87843137 0.99215686 0.99215686 0.76470588 0.03921569 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.04313725 0.09803922 0.41176471 0.3254902  0.74117647 0.74117647\n  0.89411765 0.99215686 0.98431373 0.74117647 0.74117647 0.85490196\n  0.99215686 0.99215686 0.82352941 0.10588235 0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.16470588 0.45490196\n  0.67843137 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n  0.99215686 0.99215686 0.86666667 0.45490196 0.02745098 0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.4627451  0.99215686\n  0.99215686 0.99215686 0.99215686 0.96078431 0.83137255 0.87058824\n  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n  0.99215686 0.99215686 0.99215686 0.99215686 0.62745098 0.05882353\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.99607843 0.99215686\n  0.99215686 0.99215686 0.74117647 0.38823529 0.         0.1254902\n  0.79215686 0.99215686 0.99215686 0.99215686 0.94117647 0.47843137\n  0.47843137 0.74509804 0.99215686 0.99215686 0.99215686 0.68235294\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         1.         0.99215686\n  0.99215686 0.99215686 0.93333333 0.87058824 0.87058824 0.87058824\n  0.94509804 0.99215686 0.99215686 0.90196078 0.2745098  0.\n  0.         0.06666667 0.68627451 0.89803922 0.99215686 0.99215686\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.61960784 0.99215686\n  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n  0.99215686 0.80392157 0.41568627 0.25490196 0.         0.\n  0.         0.         0.         0.24313725 0.95686275 0.61568627\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.02352941 0.10196078\n  0.70196078 0.70196078 0.70196078 0.70196078 0.70196078 0.11764706\n  0.05882353 0.03921569 0.         0.         0.         0.\n  0.         0.         0.         0.         0.05490196 0.02352941\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]]\n","output_type":"stream"},{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f8a74a365d0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split \n#X_train,  X_test,Y_train,Y_test = train_test_split(independent_sized,dependent,test_size=0.2,random_state=42)","metadata":{"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"independent_sized.shape","metadata":{"trusted":true},"execution_count":133,"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"(42000, 28, 28)"},"metadata":{}}]},{"cell_type":"code","source":"model = keras.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=(28, 28)))\nmodel.add(keras.layers.Dense(256,activation='sigmoid'))\n# model.add(keras.layers.Dropout(0.05))\nmodel.add(keras.layers.Dense(128,activation='sigmoid'))\nmodel.add(keras.layers.Dense(10, activation='sigmoid'))\n      \nmodel.compile(\n    optimizer='sgd',\n    loss='mean_squared_error',\n    metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":156,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-156-ce7ddbea35cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'he_uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model.add(keras.layers.Dropout(0.05))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2683\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2685\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2686\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    237\u001b[0m                          \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                          \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                          str(tuple(shape)))\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv2d_3 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 784)"],"ename":"ValueError","evalue":"Input 0 of layer conv2d_3 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 784)","output_type":"error"}]},{"cell_type":"code","source":"model.fit(independent_sized, dependent_vec, verbose=True,epochs=100,validation_split=0.2)","metadata":{"trusted":true},"execution_count":155,"outputs":[{"name":"stdout","text":"Epoch 1/100\n1050/1050 [==============================] - 3s 2ms/step - loss: 0.1469 - accuracy: 0.1011 - val_loss: 0.0909 - val_accuracy: 0.1382\nEpoch 2/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0905 - accuracy: 0.1505 - val_loss: 0.0899 - val_accuracy: 0.1544\nEpoch 3/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0899 - accuracy: 0.1559 - val_loss: 0.0898 - val_accuracy: 0.1550\nEpoch 4/100\n1050/1050 [==============================] - 3s 2ms/step - loss: 0.0898 - accuracy: 0.1478 - val_loss: 0.0897 - val_accuracy: 0.1610\nEpoch 5/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0897 - accuracy: 0.1636 - val_loss: 0.0897 - val_accuracy: 0.1573\nEpoch 6/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0897 - accuracy: 0.1508 - val_loss: 0.0896 - val_accuracy: 0.1642\nEpoch 7/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0896 - accuracy: 0.1668 - val_loss: 0.0896 - val_accuracy: 0.1657\nEpoch 8/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0896 - accuracy: 0.1673 - val_loss: 0.0895 - val_accuracy: 0.1676\nEpoch 9/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0895 - accuracy: 0.1873 - val_loss: 0.0895 - val_accuracy: 0.1605\nEpoch 10/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0895 - accuracy: 0.1672 - val_loss: 0.0894 - val_accuracy: 0.1685\nEpoch 11/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0894 - accuracy: 0.1686 - val_loss: 0.0894 - val_accuracy: 0.1848\nEpoch 12/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0894 - accuracy: 0.1902 - val_loss: 0.0893 - val_accuracy: 0.1826\nEpoch 13/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0893 - accuracy: 0.1954 - val_loss: 0.0893 - val_accuracy: 0.1799\nEpoch 14/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0893 - accuracy: 0.1855 - val_loss: 0.0892 - val_accuracy: 0.1932\nEpoch 15/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0892 - accuracy: 0.2117 - val_loss: 0.0892 - val_accuracy: 0.1895\nEpoch 16/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0892 - accuracy: 0.1935 - val_loss: 0.0891 - val_accuracy: 0.2077\nEpoch 17/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0891 - accuracy: 0.2130 - val_loss: 0.0891 - val_accuracy: 0.2100\nEpoch 18/100\n1050/1050 [==============================] - 3s 2ms/step - loss: 0.0891 - accuracy: 0.2121 - val_loss: 0.0890 - val_accuracy: 0.2131\nEpoch 19/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0890 - accuracy: 0.2177 - val_loss: 0.0890 - val_accuracy: 0.2279\nEpoch 20/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0890 - accuracy: 0.2318 - val_loss: 0.0889 - val_accuracy: 0.2229\nEpoch 21/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0889 - accuracy: 0.2178 - val_loss: 0.0889 - val_accuracy: 0.2408\nEpoch 22/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0889 - accuracy: 0.2385 - val_loss: 0.0888 - val_accuracy: 0.2494\nEpoch 23/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0888 - accuracy: 0.2528 - val_loss: 0.0888 - val_accuracy: 0.2586\nEpoch 24/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0888 - accuracy: 0.2557 - val_loss: 0.0887 - val_accuracy: 0.2694\nEpoch 25/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0887 - accuracy: 0.2652 - val_loss: 0.0886 - val_accuracy: 0.2779\nEpoch 26/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0887 - accuracy: 0.2761 - val_loss: 0.0886 - val_accuracy: 0.2905\nEpoch 27/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0886 - accuracy: 0.2826 - val_loss: 0.0885 - val_accuracy: 0.2979\nEpoch 28/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0885 - accuracy: 0.2918 - val_loss: 0.0885 - val_accuracy: 0.3114\nEpoch 29/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0885 - accuracy: 0.3251 - val_loss: 0.0884 - val_accuracy: 0.3021\nEpoch 30/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0884 - accuracy: 0.3085 - val_loss: 0.0883 - val_accuracy: 0.3148\nEpoch 31/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0884 - accuracy: 0.3131 - val_loss: 0.0883 - val_accuracy: 0.3332\nEpoch 32/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0883 - accuracy: 0.3247 - val_loss: 0.0882 - val_accuracy: 0.3470\nEpoch 33/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0882 - accuracy: 0.3427 - val_loss: 0.0881 - val_accuracy: 0.3523\nEpoch 34/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0881 - accuracy: 0.3414 - val_loss: 0.0880 - val_accuracy: 0.3640\nEpoch 35/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0881 - accuracy: 0.3569 - val_loss: 0.0880 - val_accuracy: 0.3720\nEpoch 36/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0880 - accuracy: 0.3619 - val_loss: 0.0879 - val_accuracy: 0.3839\nEpoch 37/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0879 - accuracy: 0.3846 - val_loss: 0.0878 - val_accuracy: 0.3788\nEpoch 38/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0878 - accuracy: 0.3730 - val_loss: 0.0877 - val_accuracy: 0.3933\nEpoch 39/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0878 - accuracy: 0.3781 - val_loss: 0.0877 - val_accuracy: 0.4012\nEpoch 40/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0877 - accuracy: 0.4011 - val_loss: 0.0876 - val_accuracy: 0.3985\nEpoch 41/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0876 - accuracy: 0.3961 - val_loss: 0.0875 - val_accuracy: 0.4050\nEpoch 42/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0875 - accuracy: 0.3963 - val_loss: 0.0874 - val_accuracy: 0.4077\nEpoch 43/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0875 - accuracy: 0.4061 - val_loss: 0.0873 - val_accuracy: 0.4094\nEpoch 44/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0873 - accuracy: 0.3989 - val_loss: 0.0872 - val_accuracy: 0.4200\nEpoch 45/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0873 - accuracy: 0.4156 - val_loss: 0.0871 - val_accuracy: 0.4167\nEpoch 46/100\n1050/1050 [==============================] - 2s 2ms/step - loss: 0.0872 - accuracy: 0.4105 - val_loss: 0.0870 - val_accuracy: 0.4270\nEpoch 47/100\n 485/1050 [============>.................] - ETA: 1s - loss: 0.0870 - accuracy: 0.4253","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-155-89bf49b61f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindependent_sized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependent_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\nsubmission['Label']=np.argmax(model.predict(independent_test),axis=-1)\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"trusted":true},"execution_count":153,"outputs":[{"execution_count":153,"output_type":"execute_result","data":{"text/plain":"       ImageId  Label\n0            1      2\n1            2      0\n2            3      9\n3            4      2\n4            5      2\n...        ...    ...\n27995    27996      9\n27996    27997      7\n27997    27998      3\n27998    27999      9\n27999    28000      2\n\n[28000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27995</th>\n      <td>27996</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>27996</th>\n      <td>27997</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>27997</th>\n      <td>27998</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>27998</th>\n      <td>27999</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>27999</th>\n      <td>28000</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>28000 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]}]}